<!DOCTYPE html>
<html>
<head>
<style type="text/css">
body {background-color: black;}
pre {
	font-weight: normal;
	color: #bbb;
	white-space: -moz-pre-wrap;
	white-space: -o-pre-wrap;
	white-space: -pre-wrap;
	white-space: pre-wrap;
	word-wrap: break-word;
	overflow-wrap: break-word;
}
b {font-weight: normal}
b.BOLD {color: #fff}
b.ITA {font-style: italic}
b.UND {text-decoration: underline}
b.STR {text-decoration: line-through}
b.UNDSTR {text-decoration: underline line-through}
b.BLK {color: #000000}
b.RED {color: #aa0000}
b.GRN {color: #00aa00}
b.YEL {color: #aa5500}
b.BLU {color: #0000aa}
b.MAG {color: #aa00aa}
b.CYN {color: #00aaaa}
b.WHI {color: #aaaaaa}
b.HIK {color: #555555}
b.HIR {color: #ff5555}
b.HIG {color: #55ff55}
b.HIY {color: #ffff55}
b.HIB {color: #5555ff}
b.HIM {color: #ff55ff}
b.HIC {color: #55ffff}
b.HIW {color: #ffffff}
b.BBLK {background-color: #000000}
b.BRED {background-color: #aa0000}
b.BGRN {background-color: #00aa00}
b.BYEL {background-color: #aa5500}
b.BBLU {background-color: #0000aa}
b.BMAG {background-color: #aa00aa}
b.BCYN {background-color: #00aaaa}
b.BWHI {background-color: #aaaaaa}
</style>
</head>
<body>
<pre>ChangeDataFlowStreamingSpec:
+ See https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html 
A dataset that is CDC enabled
- should be created and populated
  + Given a table created with the SQL: 
<b class=YEL>CREATE TABLE ChangeDataFlowSpec (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true)</b> 
  + When we write 20 rows to ChangeDataFlowSpec 
  + And again write another 20 rows to ChangeDataFlowSpec 
  + Then the history table has 3 rows, 1 for creation and 2 for insertion 
  + And the history of the source table looks like:
+-------+-----------------------+------+--------+------------+----------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+
|version|timestamp              |userId|userName|operation   |operationParameters                                                                                             |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                            |userMetadata|engineInfo                         |
+-------+-----------------------+------+--------+------------+----------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+
|2      |2023-12-04 11:43:50.067|NULL  |NULL    |WRITE       |{mode -&gt; Append, partitionBy -&gt; []}                                                                             |NULL|NULL    |NULL     |1          |Serializable  |true         |{numFiles -&gt; 2, numOutputRows -&gt; 20, numOutputBytes -&gt; 3372}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|
|1      |2023-12-04 11:43:45.691|NULL  |NULL    |WRITE       |{mode -&gt; Append, partitionBy -&gt; []}                                                                             |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -&gt; 2, numOutputRows -&gt; 20, numOutputBytes -&gt; 3372}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|
|0      |2023-12-04 11:43:36.659|NULL  |NULL    |CREATE TABLE|{isManaged -&gt; true, description -&gt; NULL, partitionBy -&gt; [], properties -&gt; {"delta.enableChangeDataFeed":"true"}}|NULL|NULL    |NULL     |NULL       |Serializable  |true         |{}                                                          |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|
+-------+-----------------------+------+--------+------------+----------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+

 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
- should write its deltas to another table as a stream *** FAILED ***
  java.util.concurrent.ExecutionException: Boxed Exception
  at scala.concurrent.impl.Promise$.scala$concurrent$impl$Promise$$resolve(Promise.scala:99)
  at scala.concurrent.impl.Promise$Transformation.handleFailure(Promise.scala:444)
  at scala.concurrent.impl.Promise$Transformation.run(Promise.scala:506)
  at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426)
  at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)
  at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)
  at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)
  at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)
  at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:177)
  ...
  Cause: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: org.apache.spark.sql.execution.datasources.v2.csv.CSVDataSourceV2 not a subtype
  at java.base/java.util.ServiceLoader.fail(ServiceLoader.java:588)
  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNextService(ServiceLoader.java:1236)
  at java.base/java.util.ServiceLoader$LazyClassPathLookupIterator.hasNext(ServiceLoader.java:1264)
  at java.base/java.util.ServiceLoader$2.hasNext(ServiceLoader.java:1299)
  at java.base/java.util.ServiceLoader$3.hasNext(ServiceLoader.java:1384)
  at scala.collection.convert.JavaCollectionWrappers$JIteratorWrapper.hasNext(JavaCollectionWrappers.scala:46)
  at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
  at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
  at scala.collection.convert.JavaCollectionWrappers$JIterableWrapper.filterImpl(JavaCollectionWrappers.scala:83)
  at scala.collection.StrictOptimizedIterableOps.filter(StrictOptimizedIterableOps.scala:218)
  ...
  + Given a table created with the SQL: 
<b class=YEL>CREATE TABLE ChangeDataFlowStreamingSpec (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true)</b> 
  + And a sink table created with SQL: 
<b class=YEL>CREATE TABLE streamsink (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA</b> 
  + When we start streaming from ChangeDataFlowStreamingSpec to streamsink with a watermark of 4 seconds and a trigger processing time of 4000 ms 
  + And the initial count in streamsink is 0 
  + And we append 100 rows with a timestamp ranging from 2023-12-04 11:43:49.788 to 2023-12-04 11:45:28.788 
  + And we wait 4000 ms 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
- should write its deltas to another table in a batch
  + Given a sink table created with SQL: 
<b class=YEL>CREATE TABLE myDeltaTable (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA</b> 
  + When we merge on the condition <b class=CYN>ChangeDataFlowSpec.id = myDeltaTable.id</b> 
  + Then the rows in the sink file are not unique, in fact there are 40 rows 
  + And the sink table looks like this:
+---+-------+------------+----------+-----------------------+
|id |label  |partitionKey|date      |timestamp              |
+---+-------+------------+----------+-----------------------+
|0  |label_0|0           |2023-12-04|2023-12-04 11:43:28.852|
|0  |label_0|0           |2023-12-04|2023-12-04 11:43:28.852|
|1  |label_1|1           |2023-12-03|2023-12-04 11:43:29.052|
|1  |label_1|1           |2023-12-03|2023-12-04 11:43:29.052|
|2  |label_2|2           |2023-12-02|2023-12-04 11:43:29.252|
|2  |label_2|2           |2023-12-02|2023-12-04 11:43:29.252|
|3  |label_3|3           |2023-12-01|2023-12-04 11:43:29.452|
|3  |label_3|3           |2023-12-01|2023-12-04 11:43:29.452|
|4  |label_4|4           |2023-11-30|2023-12-04 11:43:29.652|
|4  |label_4|4           |2023-11-30|2023-12-04 11:43:29.652|
|5  |label_5|0           |2023-11-29|2023-12-04 11:43:29.852|
|5  |label_5|0           |2023-11-29|2023-12-04 11:43:29.852|
|6  |label_6|1           |2023-11-28|2023-12-04 11:43:30.052|
|6  |label_6|1           |2023-11-28|2023-12-04 11:43:30.052|
|7  |label_7|2           |2023-11-27|2023-12-04 11:43:30.252|
|7  |label_7|2           |2023-11-27|2023-12-04 11:43:30.252|
|8  |label_8|3           |2023-11-26|2023-12-04 11:43:30.452|
|8  |label_8|3           |2023-11-26|2023-12-04 11:43:30.452|
|9  |label_9|4           |2023-11-25|2023-12-04 11:43:30.652|
|9  |label_9|4           |2023-11-25|2023-12-04 11:43:30.652|
+---+-------+------------+----------+-----------------------+
only showing top 20 rows

 
  + See https://stackoverflow.com/questions/69562007/databricks-delta-table-merge-is-inserting-records-despite-keys-are-matching-with 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
- should write its deltas to another table as a stream !!! IGNORED !!!
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
Run completed in 30 seconds, 249 milliseconds.
Total number of tests run: 3
Suites: completed 2, aborted 0
Tests: succeeded 2, failed 1, canceled 0, ignored 1, pending 0
*** 1 TEST FAILED ***</pre>
</body>
</html>
