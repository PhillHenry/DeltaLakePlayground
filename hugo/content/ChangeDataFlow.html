<!DOCTYPE html>
<html>
<head>
<style type="text/css">
body {background-color: black;}
pre {
	font-weight: normal;
	color: #bbb;
	white-space: -moz-pre-wrap;
	white-space: -o-pre-wrap;
	white-space: -pre-wrap;
	white-space: pre-wrap;
	word-wrap: break-word;
	overflow-wrap: break-word;
}
b {font-weight: normal}
b.BOLD {color: #fff}
b.ITA {font-style: italic}
b.UND {text-decoration: underline}
b.STR {text-decoration: line-through}
b.UNDSTR {text-decoration: underline line-through}
b.BLK {color: #000000}
b.RED {color: #aa0000}
b.GRN {color: #00aa00}
b.YEL {color: #aa5500}
b.BLU {color: #0000aa}
b.MAG {color: #aa00aa}
b.CYN {color: #00aaaa}
b.WHI {color: #aaaaaa}
b.HIK {color: #555555}
b.HIR {color: #ff5555}
b.HIG {color: #55ff55}
b.HIY {color: #ffff55}
b.HIB {color: #5555ff}
b.HIM {color: #ff55ff}
b.HIC {color: #55ffff}
b.HIW {color: #ffffff}
b.BBLK {background-color: #000000}
b.BRED {background-color: #aa0000}
b.BGRN {background-color: #00aa00}
b.BYEL {background-color: #aa5500}
b.BBLU {background-color: #0000aa}
b.BMAG {background-color: #aa00aa}
b.BCYN {background-color: #00aaaa}
b.BWHI {background-color: #aaaaaa}
</style>
</head>
<body>
<pre>ChangeDataFlowSpec:
+ See https://www.databricks.com/blog/2021/06/09/how-to-simplify-cdc-with-delta-lakes-change-data-feed.html 
A dataset that is CDC enabled
- should be created and populated
  + Given a table created with the SQL: 
<b class=YEL>CREATE TABLE ChangeDataFlowSpec (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true)</b> 
  + When we write 20 rows to ChangeDataFlowSpec 
  + And again write another 20 rows to ChangeDataFlowSpec 
  + Then the history table has 3 rows, 1 for creation and 2 for insertion 
  + And the history of the source table looks like:
+-------+-----------------------+------+--------+------------+----------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+
|version|timestamp              |userId|userName|operation   |operationParameters                                                                                             |job |notebook|clusterId|readVersion|isolationLevel|isBlindAppend|operationMetrics                                            |userMetadata|engineInfo                         |
+-------+-----------------------+------+--------+------------+----------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+
|2      |2023-12-04 11:38:17.896|NULL  |NULL    |WRITE       |{mode -&gt; Append, partitionBy -&gt; []}                                                                             |NULL|NULL    |NULL     |1          |Serializable  |true         |{numFiles -&gt; 2, numOutputRows -&gt; 20, numOutputBytes -&gt; 3373}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|
|1      |2023-12-04 11:38:15    |NULL  |NULL    |WRITE       |{mode -&gt; Append, partitionBy -&gt; []}                                                                             |NULL|NULL    |NULL     |0          |Serializable  |true         |{numFiles -&gt; 2, numOutputRows -&gt; 20, numOutputBytes -&gt; 3373}|NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|
|0      |2023-12-04 11:38:06.456|NULL  |NULL    |CREATE TABLE|{isManaged -&gt; true, description -&gt; NULL, partitionBy -&gt; [], properties -&gt; {"delta.enableChangeDataFeed":"true"}}|NULL|NULL    |NULL     |NULL       |Serializable  |true         |{}                                                          |NULL        |Apache-Spark/3.5.0 Delta-Lake/3.0.0|
+-------+-----------------------+------+--------+------------+----------------------------------------------------------------------------------------------------------------+----+--------+---------+-----------+--------------+-------------+------------------------------------------------------------+------------+-----------------------------------+

 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
- should write its deltas to another table in a batch
  + Given a sink table created with SQL: 
<b class=YEL>CREATE TABLE myDeltaTable (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA</b> 
  + When we merge on the condition <b class=CYN>ChangeDataFlowSpec.id = myDeltaTable.id</b> 
  + Then the rows in the sink file are not unique, in fact there are 40 rows 
  + And the sink table looks like this:
+---+-------+------------+----------+-----------------------+
|id |label  |partitionKey|date      |timestamp              |
+---+-------+------------+----------+-----------------------+
|0  |label_0|0           |2023-12-04|2023-12-04 11:37:57.953|
|0  |label_0|0           |2023-12-04|2023-12-04 11:37:57.953|
|1  |label_1|1           |2023-12-03|2023-12-04 11:37:58.153|
|1  |label_1|1           |2023-12-03|2023-12-04 11:37:58.153|
|2  |label_2|2           |2023-12-02|2023-12-04 11:37:58.353|
|2  |label_2|2           |2023-12-02|2023-12-04 11:37:58.353|
|3  |label_3|3           |2023-12-01|2023-12-04 11:37:58.553|
|3  |label_3|3           |2023-12-01|2023-12-04 11:37:58.553|
|4  |label_4|4           |2023-11-30|2023-12-04 11:37:58.753|
|4  |label_4|4           |2023-11-30|2023-12-04 11:37:58.753|
|5  |label_5|0           |2023-11-29|2023-12-04 11:37:58.953|
|5  |label_5|0           |2023-11-29|2023-12-04 11:37:58.953|
|6  |label_6|1           |2023-11-28|2023-12-04 11:37:59.153|
|6  |label_6|1           |2023-11-28|2023-12-04 11:37:59.153|
|7  |label_7|2           |2023-11-27|2023-12-04 11:37:59.353|
|7  |label_7|2           |2023-11-27|2023-12-04 11:37:59.353|
|8  |label_8|3           |2023-11-26|2023-12-04 11:37:59.553|
|8  |label_8|3           |2023-11-26|2023-12-04 11:37:59.553|
|9  |label_9|4           |2023-11-25|2023-12-04 11:37:59.753|
|9  |label_9|4           |2023-11-25|2023-12-04 11:37:59.753|
+---+-------+------------+----------+-----------------------+
only showing top 20 rows

 
  + See https://stackoverflow.com/questions/69562007/databricks-delta-table-merge-is-inserting-records-despite-keys-are-matching-with 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
- should write its deltas to another table as a stream !!! IGNORED !!!
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
- should write its deltas to another table as a stream
  + Given a table created with the SQL: 
<b class=YEL>CREATE TABLE ChangeDataFlowStreamingSpec (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA TBLPROPERTIES (delta.enableChangeDataFeed = true)</b> 
  + And a sink table created with SQL: 
<b class=YEL>CREATE TABLE streamsink (
  id int,
  label String,
  partitionKey long,
  date Date,
  timestamp Timestamp
) USING DELTA</b> 
  + When we start streaming from ChangeDataFlowStreamingSpec to streamsink with a watermark of 4 seconds and a trigger processing time of 4000 ms 
  + And the initial count in streamsink is 0 
  + And we append 100 rows with a timestamp ranging from 2023-12-04 11:38:17.756 to 2023-12-04 11:39:56.756 
  + And we wait 4000 ms 
  + Then the final row count at Mon Dec 04 11:38:28 UTC 2023 in streamsink is 100 rows 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
+ <b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b><b class=WHI>+ </b><b class=CYN>+ </b><b class=BLU>+ </b><b class=RED>+ </b><b class=GRN>+ </b><b class=MAG>+ </b><b class=YEL>+ </b> 
Run completed in 31 seconds, 306 milliseconds.
Total number of tests run: 3
Suites: completed 2, aborted 0
Tests: succeeded 3, failed 0, canceled 0, ignored 1, pending 0
All tests passed.</pre>
</body>
</html>
